{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "512a5199",
   "metadata": {},
   "source": [
    "# Local NIM Deployment with Semantic Kernel\n",
    "\n",
    "This notebook demonstrates how to integrate a local NVIDIA Inference Microservice (NIM) deployment with Microsoft Semantic Kernel for AI-powered function calling and inference.\n",
    "\n",
    "## What is NIM?\n",
    "\n",
    "NVIDIA Inference Microservice (NIM) is a container-based inference solution that allows you to deploy and serve AI models locally or in the cloud. It provides:\n",
    "\n",
    "- **High Performance**: Optimized for NVIDIA GPUs with TensorRT acceleration\n",
    "- **Easy Deployment**: Containerized microservices for consistent deployment\n",
    "- **OpenAI-Compatible API**: Standard REST API interface for seamless integration\n",
    "- **Production Ready**: Built-in security, monitoring, and scaling capabilities\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Before running this notebook, ensure you have:\n",
    "\n",
    "1. **Local NIM deployment** running (e.g., LLaMA 3.1 8B Instruct model)\n",
    "2. **Environment variables** configured:\n",
    "   - `NIM_ENDPOINT_URL`: Your local NIM endpoint (e.g., `http://localhost:8000`)\n",
    "   - `NIM_API_KEY`: API key for authentication (if required)\n",
    "   - `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_DEPLOYMENT_NAME`, `AZURE_OPENAI_API_KEY`: For orchestrating agent\n",
    "3. **Required packages** installed: `semantic-kernel`, `openai`\n",
    "\n",
    "## Architecture Overview\n",
    "\n",
    "```\n",
    "┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐\n",
    "│  Semantic       │    │   NIM Plugin     │    │  Local NIM      │\n",
    "│  Kernel Agent   │◄──►│   (Function)     │◄──►│  Deployment     │\n",
    "│                 │    │                  │    │                 │\n",
    "└─────────────────┘    └──────────────────┘    └─────────────────┘\n",
    "        │                                              │\n",
    "        ▼                                              ▼\n",
    "┌─────────────────┐                          ┌─────────────────┐\n",
    "│ Azure OpenAI    │                          │ LLaMA 3.1 8B    │\n",
    "│ (Orchestrator)  │                          │ Instruct Model  │\n",
    "└─────────────────┘                          └─────────────────┘\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f8d4b0",
   "metadata": {},
   "source": [
    "# Configuration Section: Set Your Keys and Variables\n",
    "\n",
    "**Important**: Update the values below with your actual configuration before running the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe62ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# CONFIGURATION: UPDATE THESE VALUES FOR YOUR SETUP\n",
    "# =====================================================\n",
    "\n",
    "# Local NIM Configuration\n",
    "nim_endpoint_url = \"http://localhost:8000\"  # Your local NIM endpoint\n",
    "nim_api_key = \"your-nim-api-key\"            # Your NIM API key (or set to None if not required)\n",
    "nim_model_name = \"meta/llama-3.1-8b-instruct\"  # Model name in your NIM deployment\n",
    "\n",
    "# Azure OpenAI Configuration (for orchestration agent)\n",
    "azure_openai_endpoint = \"https://your-resource.openai.azure.com\"  # Your Azure OpenAI endpoint\n",
    "azure_openai_deployment_name = \"gpt-4\"      # Your deployment name\n",
    "azure_openai_api_key = \"your-azure-openai-api-key\"  # Your Azure OpenAI API key\n",
    "\n",
    "# Alternative: Use OpenAI instead of Azure OpenAI\n",
    "use_azure_openai = True  # Set to False to use regular OpenAI\n",
    "openai_api_key = \"your-openai-api-key\"      # Only needed if use_azure_openai = False\n",
    "\n",
    "# Advanced Configuration\n",
    "nim_max_retries = 3        # Number of retry attempts for failed requests\n",
    "nim_timeout_seconds = 30   # Timeout for NIM requests\n",
    "nim_max_tokens = 256      # Maximum tokens for NIM responses\n",
    "nim_temperature = 0.7     # Temperature for response generation\n",
    "\n",
    "print(\"✅ Configuration loaded!\")\n",
    "print(f\"🏠 NIM Endpoint: {nim_endpoint_url}\")\n",
    "print(f\"🤖 NIM Model: {nim_model_name}\")\n",
    "print(f\"🔵 Using Azure OpenAI: {use_azure_openai}\")\n",
    "\n",
    "# Validate configuration\n",
    "config_warnings = []\n",
    "if nim_endpoint_url == \"http://localhost:8000\":\n",
    "    config_warnings.append(\"⚠️  Using default NIM endpoint - update if different\")\n",
    "if nim_api_key == \"your-nim-api-key\":\n",
    "    config_warnings.append(\"⚠️  Update nim_api_key with your actual API key\")\n",
    "if use_azure_openai and azure_openai_endpoint.startswith(\"https://your-resource\"):\n",
    "    config_warnings.append(\"⚠️  Update Azure OpenAI configuration\")\n",
    "if not use_azure_openai and openai_api_key == \"your-openai-api-key\":\n",
    "    config_warnings.append(\"⚠️  Update OpenAI API key\")\n",
    "\n",
    "if config_warnings:\n",
    "    print(\"\\n📋 Configuration Warnings:\")\n",
    "    for warning in config_warnings:\n",
    "        print(f\"  {warning}\")\n",
    "else:\n",
    "    print(\"✅ All configuration looks good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2690be",
   "metadata": {},
   "source": [
    "# Section 1: Import All necessairy modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b419d8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Semantic Kernel and NIM integration\n",
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "from typing import Annotated\n",
    "\n",
    "# Semantic Kernel imports\n",
    "from semantic_kernel.connectors.ai.function_choice_behavior import FunctionChoiceBehavior\n",
    "from semantic_kernel.connectors.ai.open_ai import AzureChatCompletion, OpenAIChatCompletion\n",
    "from semantic_kernel.connectors.ai.open_ai.prompt_execution_settings.open_ai_prompt_execution_settings import (\n",
    "    OpenAIChatPromptExecutionSettings,\n",
    ")\n",
    "from semantic_kernel.contents.chat_history import ChatHistory\n",
    "from semantic_kernel.contents.function_call_content import FunctionCallContent\n",
    "from semantic_kernel.functions.kernel_arguments import KernelArguments\n",
    "from semantic_kernel.functions.kernel_function_decorator import kernel_function\n",
    "from semantic_kernel.kernel import Kernel\n",
    "\n",
    "# OpenAI client for NIM communication\n",
    "from openai import OpenAI\n",
    "from openai.types.chat import ChatCompletionUserMessageParam\n",
    "\n",
    "# Configure logging for better debugging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"✅ All libraries imported successfully!\")\n",
    "print(\"📦 Semantic Kernel version:\", \"1.0.0+\")  # Version check would be dynamic in real use\n",
    "print(\"🔗 OpenAI client ready for NIM integration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed8e034",
   "metadata": {},
   "source": [
    "# Section 2: Connect to Local NIM Deployment\n",
    "\n",
    "Configure the connection to your local NIM deployment. This includes setting up the endpoint URL, API key, and connection parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744963bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for local NIM deployment\n",
    "# Set these environment variables or update the values directly\n",
    "\n",
    "# NIM endpoint configuration\n",
    "nim_endpoint_url = \"http://localhost:8000\"\n",
    "nim_api_key = \"your-nim-api-key\"\n",
    "\n",
    "# Model configuration for NIM\n",
    "nim_model_name = \"meta/llama-3.1-8b-instruct\"  # Default LLaMA 3.1 8B model\n",
    "\n",
    "# NIM Connection Setup using configuration variables\n",
    "# Uses the variables set in the Configuration Section above\n",
    "\n",
    "nim_url = nim_endpoint_url + \"/v1\"\n",
    "\n",
    "# Test connection to NIM\n",
    "def test_nim_connection():\n",
    "    \"\"\"Test basic connectivity to the local NIM deployment.\"\"\"\n",
    "    try:\n",
    "        if nim_endpoint_url.startswith(\"http://localhost\") or nim_endpoint_url.startswith(\"http://127.0.0.1\"):\n",
    "            print(f\"🔗 Connecting to local NIM deployment at: {nim_url}\")\n",
    "        else:\n",
    "            print(f\"🌐 Connecting to remote NIM deployment at: {nim_url}\")\n",
    "        \n",
    "        # Validate configuration\n",
    "        if nim_api_key == \"your-nim-api-key\":\n",
    "            print(\"⚠️  Warning: Using default API key. Update nim_api_key in the Configuration Section.\")\n",
    "        \n",
    "        # Create OpenAI client for NIM\n",
    "        client = OpenAI(\n",
    "            base_url=nim_url,\n",
    "            api_key=nim_api_key\n",
    "        )\n",
    "        \n",
    "        print(f\"✅ NIM client configured successfully!\")\n",
    "        print(f\"📊 Model: {nim_model_name}\")\n",
    "        return client\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to configure NIM client: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Test the connection\n",
    "nim_client = test_nim_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f947a5",
   "metadata": {},
   "source": [
    "# Section 3: Run a Simple Inference Example\n",
    "\n",
    "Test your local NIM deployment with a direct inference request to verify it's working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01845e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple inference example with local NIM\n",
    "def run_simple_inference(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Run a simple inference request against the local NIM deployment.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question or prompt to send to the model\n",
    "        \n",
    "    Returns:\n",
    "        str: The model's response\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if nim_client is None:\n",
    "            return \"❌ NIM client not initialized. Please run the connection test first.\"\n",
    "        \n",
    "        print(f\"🤔 Question: {question}\")\n",
    "        print(f\"⏳ Sending request to local NIM...\")\n",
    "        \n",
    "        # Create the message in OpenAI format\n",
    "        messages = [\n",
    "            ChatCompletionUserMessageParam(role=\"user\", content=question)\n",
    "        ]\n",
    "        \n",
    "        # Send request to NIM\n",
    "        response = nim_client.chat.completions.create(\n",
    "            model=nim_model_name,\n",
    "            messages=messages,\n",
    "            max_tokens=128,\n",
    "            temperature=0.7,\n",
    "            stream=False\n",
    "        )\n",
    "        \n",
    "        # Extract and return the response\n",
    "        result = response.choices[0].message.content or \"No response from model\"\n",
    "        print(f\"🎯 Response: {result}\")\n",
    "        return result\n",
    "        \n",
    "    except Exception as e:\n",
    "        error_msg = f\"❌ Error during inference: {str(e)}\"\n",
    "        print(error_msg)\n",
    "        return error_msg\n",
    "\n",
    "# Test with a simple question\n",
    "test_question = \"What are the key benefits of using NVIDIA GPUs for AI inference?\"\n",
    "response = run_simple_inference(test_question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b726d",
   "metadata": {},
   "source": [
    "# Section 4: Create Semantic Kernel Plugin for NIM\n",
    "\n",
    "Create a Semantic Kernel plugin that wraps your local NIM deployment, enabling it to be used as a function in AI orchestration workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121c154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalNIMPlugin:\n",
    "    \"\"\"\n",
    "    A Semantic Kernel plugin that provides access to local NIM deployment.\n",
    "    Uses configuration variables set in the Configuration Section.\n",
    "    \n",
    "    This plugin wraps your local NIM instance and makes it available as a \n",
    "    Semantic Kernel function for use in AI workflows and agent conversations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize the NIM plugin using configuration variables.\n",
    "        \"\"\"\n",
    "        self.nim_url = nim_url\n",
    "        self.api_key = nim_api_key\n",
    "        self.model_name = nim_model_name\n",
    "        self.max_tokens = nim_max_tokens\n",
    "        self.temperature = nim_temperature\n",
    "        self.client = OpenAI(base_url=nim_url, api_key=nim_api_key)\n",
    "        \n",
    "    @kernel_function(\n",
    "        name=\"get_nim_response\",\n",
    "        description=\"Get a response from the local NIM deployment for any question or task\"\n",
    "    )\n",
    "    def get_nim_response(\n",
    "        self, \n",
    "        question: Annotated[str, \"The question or prompt to send to the NIM model\"]\n",
    "    ) -> Annotated[str, \"The response from the NIM model\"]:\n",
    "        \"\"\"\n",
    "        Get a response from the local NIM deployment.\n",
    "        \n",
    "        Args:\n",
    "            question (str): The question or prompt to process\n",
    "            \n",
    "        Returns:\n",
    "            str: The model's response\n",
    "        \"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Processing NIM request: {question[:50]}...\")\n",
    "            \n",
    "            # Clean the prompt (remove references to the plugin name)\n",
    "            prompt = question.replace(\"nim\", \"you\").replace(\"NIM\", \"you\")\n",
    "            \n",
    "            # Validate inputs\n",
    "            if not prompt.strip():\n",
    "                return \"❗ Please provide a valid question or prompt.\"\n",
    "            \n",
    "            # Check configuration\n",
    "            if self.api_key == \"your-nim-api-key\":\n",
    "                return \"❗ Please update nim_api_key in the Configuration Section\"\n",
    "            \n",
    "            logger.info(f\"Sending request to NIM: {self.nim_url}\")\n",
    "            \n",
    "            # Create message in OpenAI format\n",
    "            messages = [\n",
    "                ChatCompletionUserMessageParam(role=\"user\", content=prompt)\n",
    "            ]\n",
    "            \n",
    "            # Send request to local NIM\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=self.model_name,\n",
    "                messages=messages,\n",
    "                max_tokens=self.max_tokens,\n",
    "                temperature=self.temperature,\n",
    "                stream=False\n",
    "            )\n",
    "            \n",
    "            # Extract and return response\n",
    "            result = response.choices[0].message.content or \"No response from model\"\n",
    "            logger.info(f\"Received response from NIM: {result[:50]}...\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            error_msg = f\"❗ Error calling local NIM: {str(e)}\"\n",
    "            logger.error(error_msg)\n",
    "            return error_msg\n",
    "\n",
    "# Create the NIM plugin instance using configuration variables\n",
    "print(\"🔧 Creating Local NIM Plugin...\")\n",
    "local_nim_plugin = LocalNIMPlugin()\n",
    "print(\"✅ Local NIM Plugin created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
